{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Redimensionar a 224x224\n",
    "    transforms.ToTensor(),  # Convertir a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizar\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Obtener el directorio actual\n",
    "current_directory = os.getcwd()\n",
    "data_directory = current_directory[:-3] + 'data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar ResNet preentrenado\n",
    "resnet = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "resnet = nn.Sequential(*list(resnet.children())[:-1])  # Quitar la última capa de clasificación\n",
    "resnet.eval()\n",
    "\n",
    "# Transformación para las imágenes\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión de las características de la imagen: torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo: Extraer características de una imagen\n",
    "image = Image.open(f'{data_directory}images\\\\images\\\\1.png')\n",
    "image_tensor = transform(image).unsqueeze(0)  # Añadir dimensión de batch\n",
    "with torch.no_grad():\n",
    "    image_features = resnet(image_tensor).squeeze()  # Embedding de la imagen\n",
    "print(\"Dimensión de las características de la imagen:\", image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class MultiLabelImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Ruta del archivo CSV con los nombres de archivo y etiquetas.\n",
    "            img_dir (str): Ruta de la carpeta donde están las imágenes.\n",
    "            transform (callable, optional): Transformaciones para aplicar a las imágenes.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Obtener el ID del archivo desde el CSV\n",
    "        img_id = self.data.iloc[idx, 0]  # Primera columna del CSV\n",
    "        \n",
    "        # Verificar extensiones posibles si no está incluida en el ID\n",
    "        extensions = [\"\", \".jpg\", \".png\", \".tif\"]\n",
    "        img_path = None\n",
    "\n",
    "        for ext in extensions:\n",
    "            temp_path = os.path.join(self.img_dir, img_id + ext)\n",
    "            if os.path.exists(temp_path):\n",
    "                img_path = temp_path\n",
    "                break\n",
    "\n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(\n",
    "                f\"No se encontró la imagen para el ID '{img_id}' con las extensiones {extensions} en {self.img_dir}.\"\n",
    "            )\n",
    "\n",
    "        # Cargar la imagen\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Convertir a RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Etiquetas multi-label\n",
    "        labels = torch.tensor(self.data.iloc[idx, 1:].values.astype(float))\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Redimensionar a 224x224\n",
    "    transforms.ToTensor(),  # Convertir a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizar\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Rutas de los archivos\n",
    "train_csv = f\"{data_directory}train_data.csv\"\n",
    "test_csv = f\"{data_directory}test_data.csv\"\n",
    "img_dir = f\"{data_directory}images/images\"\n",
    "\n",
    "# Crear datasets\n",
    "train_dataset = MultiLabelImageDataset(csv_file=train_csv, img_dir=img_dir, transform=transform)\n",
    "test_dataset = MultiLabelImageDataset(csv_file=test_csv, img_dir=img_dir, transform=transform)\n",
    "\n",
    "# Crear DataLoaders\n",
    "batch_size = 32  # Tamaño de los mini-lotes\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.1):\n",
    "        super(AdaptiveAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        # Proyecciones lineales para Q, K, V\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Parámetro adicional para la máscara adaptativa\n",
    "        self.adaptive_mask = nn.Parameter(torch.zeros(nhead, d_model // nhead))\n",
    "\n",
    "        # Softmax para calcular atención\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Dropout y proyección final\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Calcular Q, K, V\n",
    "        Q = self.query(x).view(batch_size, seq_len, self.nhead, -1).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_len, self.nhead, -1).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_len, self.nhead, -1).transpose(1, 2)\n",
    "\n",
    "        # Escalar y calcular similitud\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_model ** 0.5)\n",
    "\n",
    "        # Aplicar máscara adaptativa\n",
    "        scores = scores + self.adaptive_mask.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "        # Aplicar máscara si es necesario\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Calcular atención\n",
    "        attention = self.softmax(scores)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        # Combinar valores\n",
    "        x = torch.matmul(attention, V).transpose(1, 2).contiguous()\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class AdaptiveTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super(AdaptiveTransformerEncoderLayer, self).__init__()\n",
    "        self.adaptive_attention = AdaptiveAttention(d_model, nhead, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        # Atención adaptativa\n",
    "        src2 = self.adaptive_attention(src, mask=src_mask)\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # Feedforward\n",
    "        src2 = self.feed_forward(src)\n",
    "        src = src + self.dropout(src2)\n",
    "        src = self.norm2(src)\n",
    "\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jomunozf\\Documents\\GitHub\\Transformer_AdapAttention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import TransformerEncoder\n",
    "\n",
    "# Parámetros del Transformer\n",
    "embedding_dim = 2048  # Dimensión de salida de ResNet\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "\n",
    "# Construir TransformerEncoder con capas personalizadas\n",
    "encoder_layers = [AdaptiveTransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads) for _ in range(num_layers)]\n",
    "transformer_encoder = TransformerEncoder(nn.ModuleList(encoder_layers), num_layers=num_layers)\n",
    "\n",
    "# Reemplaza el forward del Transformer\n",
    "class AdaptiveAttentionClassifier(nn.Module):\n",
    "    def __init__(self, image_feature_dim, num_classes):\n",
    "        super(AdaptiveAttentionClassifier, self).__init__()\n",
    "        self.image_embed = nn.Linear(image_feature_dim, 512)  # Embedding de características\n",
    "        self.transformer_encoder = nn.ModuleList([\n",
    "            AdaptiveTransformerEncoderLayer(d_model=512, nhead=8) for _ in range(6)\n",
    "        ])\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, image_features):\n",
    "        # Proyección de características\n",
    "        image_embeddings = self.image_embed(image_features)\n",
    "\n",
    "        # Pasar por las capas del Transformer manualmente\n",
    "        for layer in self.transformer_encoder:\n",
    "            image_embeddings = layer(image_embeddings.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Clasificador final\n",
    "        output = self.mlp(image_embeddings)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding de etiqueta: torch.Size([1, 128])\n",
      "Embedding de estado: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "# Crear embeddings aleatorios para etiquetas y estados\n",
    "label_embeddings = torch.nn.Embedding(num_embeddings=5, embedding_dim=128)  # Ejemplo: 5 etiquetas (DR, NORMAL, etc.)\n",
    "state_embeddings = torch.nn.Embedding(num_embeddings=3, embedding_dim=128)  # Ejemplo: 3 estados (YES, NO, ?)\n",
    "\n",
    "# Etiqueta y estado de entrada\n",
    "label = torch.tensor([0])  # \"DR\"\n",
    "state = torch.tensor([1])  # \"NO\"\n",
    "\n",
    "# Obtener embeddings\n",
    "label_embed = label_embeddings(label)\n",
    "state_embed = state_embeddings(state)\n",
    "print(\"Embedding de etiqueta:\", label_embed.shape)\n",
    "print(\"Embedding de estado:\", state_embed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión de los embeddings combinados: torch.Size([2304])\n"
     ]
    }
   ],
   "source": [
    "# Concatenar embeddings\n",
    "combined_embeddings = torch.cat([image_features, label_embed.squeeze(), state_embed.squeeze()], dim=-1)\n",
    "print(\"Dimensión de los embeddings combinados:\", combined_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jomunozf\\Documents\\GitHub\\Transformer_AdapAttention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ModuleList' object has no attribute 'self_attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m resnet(images)\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     27\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\jomunozf\\Documents\\GitHub\\Transformer_AdapAttention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jomunozf\\Documents\\GitHub\\Transformer_AdapAttention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[16], line 34\u001b[0m, in \u001b[0;36mAdaptiveAttentionClassifier.forward\u001b[1;34m(self, image_features)\u001b[0m\n\u001b[0;32m     31\u001b[0m image_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_embed(image_features)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Pasar por el Transformer\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m encoded_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Clasificador final\u001b[39;00m\n\u001b[0;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(encoded_features\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\jomunozf\\Documents\\GitHub\\Transformer_AdapAttention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jomunozf\\Documents\\GitHub\\Transformer_AdapAttention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\jomunozf\\Documents\\GitHub\\Transformer_AdapAttention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:431\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    429\u001b[0m why_not_sparsity_fast_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    430\u001b[0m str_first_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.layers[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 431\u001b[0m batch_first \u001b[38;5;241m=\u001b[39m \u001b[43mfirst_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241m.\u001b[39mbatch_first\n\u001b[0;32m    432\u001b[0m is_fastpath_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmha\u001b[38;5;241m.\u001b[39mget_fastpath_enabled()\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fastpath_enabled:\n",
      "File \u001b[1;32mc:\\Users\\jomunozf\\Documents\\GitHub\\Transformer_AdapAttention\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ModuleList' object has no attribute 'self_attn'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Modelo (asegúrate de tener tu modelo configurado correctamente)\n",
    "model = AdaptiveAttentionClassifier(image_feature_dim=2048, num_classes=train_dataset[0][1].size(0))\n",
    "model.to(device)\n",
    "\n",
    "# Configuración de optimizador y función de pérdida\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # Para clasificación multi-label\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 1  # Número de épocas\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Entrenamiento\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Extraer características de ResNet\n",
    "        with torch.no_grad():\n",
    "            image_features = resnet(images).flatten(start_dim=1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(image_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validación\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            image_features = resnet(images).flatten(start_dim=1)\n",
    "            outputs = model(image_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Promedio de pérdidas\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(test_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos los archivos del CSV están presentes.\n"
     ]
    }
   ],
   "source": [
    "csv_file = f\"{data_directory}train_data.csv\"\n",
    "img_dir = f\"{data_directory}images/images\"\n",
    "\n",
    "data = pd.read_csv(csv_file)\n",
    "missing_files = []\n",
    "\n",
    "for img_id in data['ID']:\n",
    "    file_found = any(\n",
    "        os.path.exists(os.path.join(img_dir, img_id + ext))\n",
    "        for ext in [\"\", \".jpg\", \".png\", \".tif\"]\n",
    "    )\n",
    "    if not file_found:\n",
    "        missing_files.append(img_id)\n",
    "\n",
    "if missing_files:\n",
    "    print(\"Los siguientes archivos no se encontraron en el directorio:\")\n",
    "    print(missing_files)\n",
    "else:\n",
    "    print(\"Todos los archivos del CSV están presentes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# Cargar el modelo y el procesador\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Ejemplo: Imagen y texto de entrada\n",
    "image = Image.open(\"example.jpg\")  # Reemplaza con tu imagen\n",
    "text = [\"Este es un ejemplo\", \"Otra descripción\"]\n",
    "\n",
    "# Procesar entradas\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "# Hacer el cálculo de similitud\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # Similaridad entre imagen y texto\n",
    "probs = logits_per_image.softmax(dim=1)  # Convertir a probabilidades\n",
    "\n",
    "print(\"Probs:\", probs)\n",
    "\n",
    "##################################################################################\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Cargar el modelo AlexNet preentrenado\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet.eval()\n",
    "\n",
    "# Preprocesamiento de la imagen\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "image = Image.open(\"path_to_your_image.jpg\")\n",
    "image_tensor = transform(image).unsqueeze(0)  # Agregar batch dimension\n",
    "\n",
    "# Extraer características de la imagen (penúltima capa)\n",
    "with torch.no_grad():\n",
    "    features = alexnet.features(image_tensor).flatten(start_dim=1)\n",
    "\n",
    "print(\"Características de la imagen:\", features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Cargar modelo CLIP\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Preprocesar imagen\n",
    "image = Image.open(\"path_to_image.jpg\")\n",
    "inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    image_features = clip_model.get_image_features(**inputs)  # Embedding de la imagen\n",
    "print(\"Dimensión de las características de la imagen:\", image_features.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
